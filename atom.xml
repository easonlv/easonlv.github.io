<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Diving into Data</title>
  
  <subtitle>略懂一些算法，只会几种编程语言，半栈工程师，不靠谱分析师。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://easonlv.github.io/"/>
  <updated>2019-08-26T03:48:40.435Z</updated>
  <id>http://easonlv.github.io/</id>
  
  <author>
    <name>Eason Lv</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Tensorflow中自定义Loss Function</title>
    <link href="http://easonlv.github.io/2019/08/25/Tensorflow%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89Loss%20Function/"/>
    <id>http://easonlv.github.io/2019/08/25/Tensorflow中自定义Loss Function/</id>
    <published>2019-08-25T15:05:38.577Z</published>
    <updated>2019-08-26T03:48:40.435Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-交叉熵损失函数"><a href="#1-交叉熵损失函数" class="headerlink" title="1.交叉熵损失函数"></a>1.交叉熵损失函数</h1><p>交叉熵H(p,q)是指按照不真实分布q来编码样本p所需的编码长度的期望。交叉熵损失是神经网络分类问题中常见的损失函数，用来判断模型对真实概率分布估计的准确程度，q(x)是预测的概率分布，p(x) 是真实的概率分布（在多分类问题的 one-hot 编码）<br>$$<br>H(p,q)=-\sum_x p(x) log\ q(x)<br>$$</p><p>二分类问题又可进一步展开为：<br>$$H(y,a)=H_y(a)=−(yloga+(1−y)log(1−a))$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y_ 真实输出值，y 预测值</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">cross_ent = -tf.reduce_mean(tf.reduce_sum(y_*tf.log(y), reduce_indices=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h1 id="2-TensorFlow交叉熵实现"><a href="#2-TensorFlow交叉熵实现" class="headerlink" title="2.TensorFlow交叉熵实现"></a>2.TensorFlow交叉熵实现</h1><p>TensorFlow中交叉熵的损失函数（loss function）主要有以下几种：</p><h2 id="2-1-tf-nn-softmax-cross-entropy-with-logits-v2"><a href="#2-1-tf-nn-softmax-cross-entropy-with-logits-v2" class="headerlink" title="2.1 tf.nn.softmax_cross_entropy_with_logits_v2"></a>2.1 tf.nn.softmax_cross_entropy_with_logits_v2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_cross_entropy_with_logits_v2</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    _sentinel=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    labels=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    logits=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    dim=<span class="number">-1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    name=None)</span>:</span></span><br></pre></td></tr></table></figure><blockquote><p>Computes softmax cross entropy between logits and labels</p></blockquote><p>原来的函数为nn.softmax_cross_entropy_with_logits，现在deprecated，位于nn_ops.py文件下。</p><p>softmax交叉熵loss，参数为网络最后一层的输出和onehot形式的标签。衡量独立互斥离散分类任务的误差，each entry is in exactly one class。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">logits = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]], tf.float64)</span><br><span class="line">y = tf.nn.softmax(logits)</span><br><span class="line">y_ = tf.constant([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]], tf.float64)</span><br><span class="line"></span><br><span class="line">cross_ent = -tf.reduce_sum(y_ * tf.log(y))</span><br><span class="line">cross_ent2 = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(<span class="string">'cross_ent: '</span>, sess.run(cross_ent))</span><br><span class="line">    print(<span class="string">'cross_ent2: '</span>, sess.run(cross_ent2))</span><br></pre></td></tr></table></figure><h2 id="2-2-tf-nn-sparse-softmax-cross-entropy-with-logits"><a href="#2-2-tf-nn-sparse-softmax-cross-entropy-with-logits" class="headerlink" title="2.2 tf.nn.sparse_softmax_cross_entropy_with_logits"></a>2.2 tf.nn.sparse_softmax_cross_entropy_with_logits</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sparse_softmax_cross_entropy_with_logits</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    _sentinel=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    labels=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    logits=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    name=None)</span>:</span></span><br></pre></td></tr></table></figure><blockquote><p>Computes sparse softmax cross entropy between logits and labels</p></blockquote><p>这个函数和上面的区别就是labels参数应该是没有经过onehot的标签，其余的都是一样的。</p><h2 id="2-3-tf-nn-sigmoid-cross-entropy-with-logits"><a href="#2-3-tf-nn-sigmoid-cross-entropy-with-logits" class="headerlink" title="2.3 tf.nn. sigmoid_cross_entropy_with_logits"></a>2.3 tf.nn. sigmoid_cross_entropy_with_logits</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_cross_entropy_with_logits</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    _sentinel=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    labels=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    logits=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    name=None)</span>:</span></span><br></pre></td></tr></table></figure><blockquote><p>Computes sigmoid cross entropy given logits</p></blockquote><p>sigmoid交叉熵loss，与softmax不同的是，该函数首先进行sigmoid操作之后计算交叉熵的损失函数。衡量独立互斥离散分类任务的误差，可以处理多分类任务中的多目标检测，多标记学习等。</p><p>实现方式如下：</p><blockquote><p>let <code>x = logits</code>, <code>z = labels</code><br>The logistic loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))</p></blockquote><h2 id="2-4-tf-nn-weighted-cross-entropy-with-logits"><a href="#2-4-tf-nn-weighted-cross-entropy-with-logits" class="headerlink" title="2.4 tf.nn.weighted_cross_entropy_with_logits"></a>2.4 tf.nn.weighted_cross_entropy_with_logits</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted_cross_entropy_with_logits</span><span class="params">(targets, logits, pos_weight, name=None)</span>:</span></span><br></pre></td></tr></table></figure><p>交叉熵损失通常定义为:<br>$$<br>loss_usual = targets <em> -log(sigmoid(logits)) + (1 - targets) </em> -log(1 - sigmoid(logits))<br>$$</p><p>而这种正例加权的损失函数定义为:<br>$$<br>loss_weight = targets <em> -log(sigmoid(logits)) </em> pos_weight + (1 - targets) * -log(1 - sigmoid(logits))<br>$$</p><p>当设置参数 pos_weight&gt;1 时，减少了False Negative的数量，增加了召回率。<br>当设置参数 pos_weight&lt;1 时，减少了False Positive的数量，增加了准确率。</p><p>实现方式如下：</p><blockquote><p>let <code>x = logits</code>, <code>z = targets</code>, <code>q = pos_weight</code>,<br>setting <code>l = (1 + (q - 1) * z)</code><br>loss = (1 - z) <em> x + l </em> (log(1 + exp(-abs(x))) + max(-x, 0))</p></blockquote><h1 id="3-TensorFlow自定义损失函数"><a href="#3-TensorFlow自定义损失函数" class="headerlink" title="3 TensorFlow自定义损失函数"></a>3 TensorFlow自定义损失函数</h1><p>TensorFlow 不仅支持经典的损失的损失函数，还可以优化任意的自定义损失函数。有两个办法：</p><p>其一、你自己用C++写一个。你需要把tensorflow的源代码下载下来，然后自己用C++写一个函数。可参考Adding an op。</p><p>其二、你把你自己定义的损失函数用tensorflow中的标准函数表示出来。比如你需要MSE（虽然这个已经有了），可以写成 loss = tf.reduce_mean(tf.square(tf.sub(y_real, y_pred)))</p><p>例子来源于《Tensorflow实战Google深度学习框架》<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">使用交叉熵损失函数的神经网络分类demo</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">8</span></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="literal">None</span>,<span class="number">2</span>),name=<span class="string">'x_input'</span>)</span><br><span class="line">y_=tf.placeholder(tf.float32,shape=(<span class="literal">None</span>,<span class="number">1</span>),name=<span class="string">'y_input'</span>)</span><br><span class="line">w2=tf.Variable(tf.random_normal([<span class="number">3</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line">a=tf.matmul(x,w1)</span><br><span class="line">y=tf.matmul(a,w2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数和反向传播算法</span></span><br><span class="line"><span class="comment">#自定义交叉熵</span></span><br><span class="line">cross_entropy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,<span class="number">1e-10</span>,<span class="number">1.0</span>)))</span><br><span class="line">train_step=tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(cross_entropy)</span><br><span class="line"><span class="comment">#模拟生成训练集</span></span><br><span class="line">rdm=RandomState(<span class="number">1</span>)</span><br><span class="line">dataset_size=<span class="number">128</span></span><br><span class="line">X=rdm.rand(dataset_size,<span class="number">2</span>)</span><br><span class="line">Y=[[int(x1+x2&lt;<span class="number">1</span>)]<span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"><span class="comment">#0、1二分类</span></span><br><span class="line">print(X.shape)</span><br><span class="line">print(Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br><span class="line"></span><br><span class="line">    STEPS=<span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = ( i* batch_size)%dataset_size</span><br><span class="line">        end= min(start+batch_size,dataset_size)</span><br><span class="line">        <span class="comment">#通过选取的样本训练神经网络或训练参数</span></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end], y_ : Y[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#计算每隔一段时间所有数据的交叉熵并输出</span></span><br><span class="line">            total_cross_entropy=sess.run(cross_entropy,feed_dict=&#123;x:X,y_:Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training steps,cross entropy on all data is %g"</span>%(i,total_cross_entropy))</span><br><span class="line">    print(sess.run(w1))</span><br><span class="line">    print(sess.run(w2))</span><br></pre></td></tr></table></figure></p><p>对于理想的分类问题和回归问题，可采用交叉熵或者MSE损失函数，但是对于一些实际的问题，理想的损失函数可能在表达上不能完全表达损失情况，以至于影响对结果的优化。例如：对于产品销量预测问题，表面上是一个回归问题，可使用MSE损失函数。可实际情况下，当预测值大于实际值时，损失值应是正比于商品成本的函数；当预测值小于实际值，损失值是正比于商品利润的函数，多数情况下商品成本和利润是不对等的。自定义损失函数如下:<br>$$<br>Loss(y,y’)=\sum_{i=1}^n f(y_i,y’_i),<br>f(x,y)= \left\{<br>\begin{matrix}<br>a(x-y), x &gt; y \\<br>b(y-x), x \leqslant y<br>\end{matrix} \right.<br>$$</p><p>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">使用自定义损失函数的神经网络回归demo</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> RandomState</span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">8</span></span><br><span class="line"></span><br><span class="line">x=tf.placeholder(tf.float32,shape=(<span class="literal">None</span>,<span class="number">2</span>),name=<span class="string">'x_input'</span>)</span><br><span class="line">y_=tf.placeholder(tf.float32,shape=(<span class="literal">None</span>,<span class="number">1</span>),name=<span class="string">'y_input'</span>)</span><br><span class="line">w1=tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">1</span>],stddev=<span class="number">1</span>,seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment">#预测值</span></span><br><span class="line">y=tf.matmul(x,w1)</span><br><span class="line"></span><br><span class="line">loss_less=<span class="number">10</span></span><br><span class="line">loss_more=<span class="number">1</span></span><br><span class="line"><span class="comment">#定义损失函数和反向传播算法</span></span><br><span class="line"><span class="comment">#自定义损失函数</span></span><br><span class="line">loss=tf.reduce_sum(tf.where(tf.greater(y,y_),(y-y_)*loss_more,(y_-y)*loss_less))</span><br><span class="line"></span><br><span class="line">train_step=tf.train.AdamOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line">rdm=RandomState(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">dataset_size=<span class="number">128</span></span><br><span class="line">X=rdm.rand(dataset_size,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">Y=[[x1+x2+rdm.rand()/<span class="number">10.0</span><span class="number">-0.05</span>]<span class="keyword">for</span> (x1,x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op=tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS=<span class="number">5000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * batch_size) % dataset_size</span><br><span class="line">        end= min(start+batch_size,dataset_size)</span><br><span class="line">        <span class="comment">#通过选取的样本训练神经网络或训练参数</span></span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x: X[start:end], y_ : Y[start:end]&#125;)</span><br><span class="line">    print(sess.run(w1))</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-交叉熵损失函数&quot;&gt;&lt;a href=&quot;#1-交叉熵损失函数&quot; class=&quot;headerlink&quot; title=&quot;1.交叉熵损失函数&quot;&gt;&lt;/a&gt;1.交叉熵损失函数&lt;/h1&gt;&lt;p&gt;交叉熵H(p,q)是指按照不真实分布q来编码样本p所需的编码长度的期望。交叉熵损失是
      
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://easonlv.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>CCF推荐 国际学术会议</title>
    <link href="http://easonlv.github.io/2019/08/22/CCF%E6%8E%A8%E8%8D%90-%E5%9B%BD%E9%99%85%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/"/>
    <id>http://easonlv.github.io/2019/08/22/CCF推荐-国际学术会议/</id>
    <published>2019-08-22T07:31:17.000Z</published>
    <updated>2019-08-22T14:14:38.960Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据挖掘-内容检索"><a href="#1-数据挖掘-内容检索" class="headerlink" title="1.数据挖掘/内容检索"></a>1.数据挖掘/内容检索</h1><p><a href="https://www.ccf.org.cn/xspj/sjk/sjwj/nrjs/" target="_blank" rel="noopener">https://www.ccf.org.cn/xspj/sjk/sjwj/nrjs/</a></p><h2 id="A类"><a href="#A类" class="headerlink" title="A类"></a>A类</h2><table><tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>SIGMOD</td><td>ACM Conference on Management of Data</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/sigmod/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/sigmod/</a></td></tr><tr><td>2</td><td>SIGKDD</td><td>ACM Knowledge Discovery and Data Mining</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/kdd/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/kdd/</a></td></tr><tr><td>3</td><td>ICDE</td><td>IEEE International Conference on Data Engineering</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icde/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icde/</a></td></tr><tr><td>4</td><td>SIGIR</td><td>International Conference on Research on Development in Information Retrieval</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/sigir/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/sigir/</a></td></tr><tr><td>5</td><td>VLDB</td><td>International Conference on Very Large Data Bases</td>    <td>Morgan Kaufmann</td>    <td><a href="http://dblp.uni-trier.de/db/conf/vldb/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/vldb/</a></td></tr></table><h2 id="B类"><a href="#B类" class="headerlink" title="B类"></a>B类</h2><table><tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>CIKM</td><td>ACM International Conference on Information and Knowledge Management</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/cikm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/cikm/</a></td></tr><tr><td>2</td><td>WSDM</td><td>ACM International Conference on Web Search and Data Mining</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/wsdm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/wsdm/</a></td></tr><tr><td>3</td><td>PODS</td><td>ACM Symposium on Principles of Database Systems</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/pods/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/pods/</a></td></tr><tr><td>4</td><td>DASFAA</td><td>Database Systems for Advanced Applications</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/dasfaa/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/dasfaa/</a></td></tr><tr><td>5</td><td>ECML-PKDD</td><td>European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ecml/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ecml/</a></td></tr><tr><td>6</td><td>ISWC</td><td>IEEE International Semantic Web Conference</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/semweb/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/semweb/</a></td></tr><tr><td>7</td><td>ICDM</td><td>International Conference on Data Mining</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icdm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icdm/</a></td></tr><tr><td>8</td><td>ICDT</td><td>International Conference on Database Theory</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icdt/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icdt/</a></td></tr><tr><td>9</td><td>EDBT</td><td>International Conference on Extending DB Technology</td>    <td>Springer </td>    <td><a href="http://dblp.uni-trier.de/db/conf/edbt/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/edbt/</a></td></tr><tr><td>10</td><td>CIDR</td><td>International Conference on Innovative Data Systems Research</td>    <td>Online Proceeding</td>    <td><a href="http://dblp.uni-trier.de/db/conf/cidr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/cidr/</a></td></tr><tr><td>11</td><td>SDM</td><td>SIAM International Conference on Data Mining</td>    <td>SIAM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/sdm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/sdm/</a></td></tr>  </table><h2 id="C类"><a href="#C类" class="headerlink" title="C类"></a>C类</h2><table><tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>APWeb</td><td>Asia Pacific Web Conference</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/apweb/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/apweb/</a></td></tr><tr><td>2</td><td>DEXA</td><td>Database and Expert System Applications</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/dexa/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/dexa/</a></td></tr><tr><td>3</td><td>ECIR</td><td>European Conference on IR Research</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ecir/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ecir/</a></td></tr><tr><td>4</td><td>ESWC</td><td>Extended Semantic Web Conference</td>    <td>Elsevier</td>    <td><a href="http://dblp.uni-trier.de/db/conf/esws/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/esws/</a></td></tr><tr><td>5</td><td>WebDB</td><td>International ACM Workshop on Web and Databases</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/webdb/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/webdb/</a></td></tr><tr><td>6</td><td>ER</td><td>International Conference on Conceptual Modeling</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/er/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/er/</a></td></tr><tr><td>7</td><td>MDM</td><td>International Conference on Mobile Data Management</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/mdm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/mdm/</a></td></tr><tr><td>8</td><td>SSDBM</td><td>International Conference on Scientific and Statistical DB Management</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ssdbm/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ssdbm/</a></td></tr><tr><td>9</td><td>WAIM</td><td>International Conference on Web Age Information Management</td>    <td>Springer </td>    <td><a href="http://dblp.uni-trier.de/db/conf/waim/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/waim/</a></td></tr><tr><td>10</td><td>SSTD</td><td>International Symposium on Spatial and Temporal Databases</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ssd/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ssd/</a></td></tr><tr><td>11</td><td>PAKDD</td><td>Pacific-Asia Conference on Knowledge Discovery and Data Mining</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/pakdd/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/pakdd/</a></td></tr><tr><td>12</td><td>WISE</td><td>Web Information Systems Engineering</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/wise/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/wise/</a></td></tr>  </table><h1 id="2-人工智能"><a href="#2-人工智能" class="headerlink" title="2.人工智能"></a>2.人工智能</h1><p><a href="https://www.ccf.org.cn/xspj/rgzn/" target="_blank" rel="noopener">https://www.ccf.org.cn/xspj/rgzn/</a></p><h2 id="A类-1"><a href="#A类-1" class="headerlink" title="A类"></a>A类</h2><table><tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>AAAI</td><td>AAAI Conference on Artificial Intelligence</td>    <td>AAAI</td>    <td><a href="http://dblp.uni-trier.de/db/conf/aaai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/aaai/</a></td></tr><tr><td>2</td><td>NeurIPS</td><td>Annual Conference on Neural Information Processing Systems</td>    <td>MIT Press</td>    <td><a href="http://dblp.uni-trier.de/db/conf/nips/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/nips/</a></td></tr><tr><td>3</td><td>ACL</td><td>Annual Meeting of the Association for Computational Linguistics</td>    <td>ACL</td>    <td><a href="http://dblp.uni-trier.de/db/conf/acl/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/acl/</a></td></tr><tr><td>4</td><td>CVPR</td><td>IEEE Conference on Computer Vision and Pattern Recognition</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/cvpr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/cvpr/</a></td></tr><tr><td>5</td><td>ICCV</td><td>International Conference on Computer Vision</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/iccv/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/iccv/</a></td></tr><tr><td>6</td><td>ICML</td><td>International Conference on Machine Learning</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icml/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icml/</a></td></tr><tr><td>7</td><td>IJCAI</td><td>International Joint Conference on Artificial Intelligence</td>    <td>Morgan Kaufmann</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ijcai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ijcai/</a></td></tr></table><h2 id="B类-1"><a href="#B类-1" class="headerlink" title="B类"></a>B类</h2><table><tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>COLT</td><td>Annual Conference on Computational Learning Theory</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/colt/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/colt/</a></td></tr><tr><td>2</td><td>EMNLP</td><td>Conference on Empirical Methods in Natural Language Processing</td>    <td>ACL</td>    <td><a href="http://dblp.uni-trier.de/db/conf/emnlp/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/emnlp/</a></td></tr><tr><td>3</td><td>ECAI</td><td>European Conference on Artificial Intelligence</td>    <td>IOS Press</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ecai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ecai/</a></td></tr><tr><td>4</td><td>ECCV</td><td>European Conference on Computer Vision</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/eccv/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/eccv/</a></td></tr><tr><td>5</td><td>ICRA</td><td>IEEE International Conference on Robotics and Automation</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icra/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icra/</a></td></tr><tr><td>6</td><td>ICAPS</td><td>International Conference on Automated Planning and Scheduling</td>    <td>AAAI</td>    <td><a href="http://dblp.uni-trier.de/db/conf/aips/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/aips/</a></td></tr><tr><td>7</td><td>ICCBR</td><td>International Conference on Case-Based Reasoning and Development</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/iccbr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/iccbr/</a></td></tr><tr><td>8</td><td>COLING</td><td>International Conference on Computational Linguistics</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/coling/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/coling/</a></td></tr><tr><td>9</td><td>KR</td><td>International Conference on Principles of Knowledge Representation and Reasoning</td>    <td>Morgan Kaufmann</td>    <td><a href="http://dblp.uni-trier.de/db/conf/kr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/kr/</a></td></tr><tr><td>10</td><td>UAI</td><td>International Conference on Uncertainty in Artificial Intelligence</td>    <td>AUAI</td>    <td><a href="http://dblp.uni-trier.de/db/conf/uai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/uai/</a></td></tr><tr><td>11</td><td>AAMAS</td><td>International Joint Conference on Autonomous Agents and Multi-agent Systems</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/atal/index.html" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/atal/index.html</a></td></tr><tr><td>12</td><td>PPSN</td><td>Parallel Problem Solving from Nature</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ppsn/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ppsn/</a></td></tr>  </table><h2 id="C类-1"><a href="#C类-1" class="headerlink" title="C类"></a>C类</h2><table>  <tr><td>序号</td><td>刊物名称</td><td>刊物全称</td>    <td>出版社</td>    <td>地址</td></tr><tr><td>1</td><td>AISTATS</td><td>Artificial Intelligence and Statistics</td>    <td>JMLR</td>    <td><a href="http://dblp.uni-trier.de/db/conf/aistats/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/aistats/</a></td></tr><tr><td>2</td><td>ACCV</td><td>Asian Conference on Computer Vision</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/accv/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/accv/</a></td></tr><tr><td>3</td><td>ACML</td><td>Asian Conference on Machine Learning</td>    <td>JMLR</td>    <td><a href="http://dblp.uni-trier.de/db/conf/acml/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/acml/</a></td></tr><tr><td>4</td><td>BMVC</td><td>British Machine Vision Conference</td>    <td> British Machine Vision Association</td>    <td><a href="http://dblp.uni-trier.de/db/conf/bmvc/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/bmvc/</a></td></tr><tr><td>5</td><td>NLPCC</td><td>CCF International Conference on Natural Language Processing and Chinese Computing</td>    <td>Springer</td>    <td><a href="https://dblp.uni-trier.de/db/conf/nlpcc/" target="_blank" rel="noopener">https://dblp.uni-trier.de/db/conf/nlpcc/</a></td></tr><tr><td>6</td><td>CoNLL</td><td>Conference on Computational Natural Language Learning</td>    <td>Association for Computational Linguistics</td>    <td><a href="http://dblp.uni-trier.de/db/conf/conll" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/conll</a></td></tr><tr><td>7</td><td>GECCO</td><td>Genetic and Evolutionary Computation Conference</td>    <td>ACM</td>    <td><a href="http://dblp.uni-trier.de/db/conf/gecco/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/gecco/</a></td></tr><tr><td>8</td><td>ICTAI</td><td>IEEE International Conference on Tools with Artificial Intelligence</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ictai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ictai/</a></td></tr><tr><td>9</td><td>IROS</td><td>IEEE\RSJ International Conference on Intelligent Robots and Systems</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/iros/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/iros/</a></td></tr><tr><td>10</td><td>ALT</td><td>International Conference on Algorithmic Learning Theory</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/alt/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/alt/</a></td></tr><tr><td>11</td><td>ICANN</td><td>International Conference on Artificial Neural Networks</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icann/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icann/</a></td></tr><tr><td>12</td><td>FG</td><td>International Conference on Automatic Face and Gesture Recognition</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/fgr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/fgr/</a></td></tr><tr><td>13</td><td>ICDAR</td><td>International Conference on Document Analysis and Recognition</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icdar/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icdar/</a></td></tr><tr><td>14</td><td>ILP</td><td>International Conference on Inductive Logic Programming</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ilp/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ilp/</a></td></tr><tr><td>15</td><td>KSEM</td><td>International conference on Knowledge Science,Engineering and Management</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ksem/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ksem/</a></td></tr><tr><td>16</td><td>ICONIP</td><td>International Conference on Neural Information Processing</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/iconip/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/iconip/</a></td></tr><tr><td>17</td><td>ICPR</td><td>International Conference on Pattern Recognition</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icpr/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icpr/</a></td></tr><tr><td>18</td><td>ICB</td><td>International Joint Conference on Biometrics</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/icb/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/icb/</a></td></tr><tr><td>19</td><td>IJCNN</td><td>International Joint Conference on Neural Networks</td>    <td>IEEE</td>    <td><a href="http://dblp.uni-trier.de/db/conf/ijcnn/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/ijcnn/</a></td></tr><tr><td>20</td><td>PRICAI</td><td>Pacific Rim International Conference on Artificial Intelligence</td>    <td>Springer</td>    <td><a href="http://dblp.uni-trier.de/db/conf/pricai/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/pricai/</a></td></tr><tr><td>21</td><td>NAACL</td><td>The Annual Conference of the North  American Chapter of the Association for Computational Linguistics</td>    <td>NAACL</td>    <td><a href="http://dblp.uni-trier.de/db/conf/naacl/" target="_blank" rel="noopener">http://dblp.uni-trier.de/db/conf/naacl/</a></td></tr></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-数据挖掘-内容检索&quot;&gt;&lt;a href=&quot;#1-数据挖掘-内容检索&quot; class=&quot;headerlink&quot; title=&quot;1.数据挖掘/内容检索&quot;&gt;&lt;/a&gt;1.数据挖掘/内容检索&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.ccf.org.cn/xs
      
    
    </summary>
    
    
      <category term="paper" scheme="http://easonlv.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Applying Deep Learning To Airbnb Search》</title>
    <link href="http://easonlv.github.io/2019/03/27/KDD2018_Applying-Deep-Learning-To-Airbnb-Search/"/>
    <id>http://easonlv.github.io/2019/03/27/KDD2018_Applying-Deep-Learning-To-Airbnb-Search/</id>
    <published>2019-03-27T11:10:52.000Z</published>
    <updated>2019-08-25T15:05:13.548Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://static001.infoq.cn/resource/image/38/e2/383d9bff2837a4931e3703508d7a01e2.jpg" alt="airbnb"></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>在Airbnb机器学习应用中，搜索排序是获得最大成功的案例之一。大多数初始收益是来自于梯度提升决策树模型（即GBDT）。 然而，随着时间的推移收益趋于稳定。 本文讨论了我们尝试应用神经网络的这样一个突破性的工作。我们提出的观点并非旨在推动新建模技术的前沿。 相反，我们的故事是我们发现在将神经网络应用于现实生活产品时有用的元素。对我们来说是深度学习的曲线比较陡。但对于开始类似工作的其他团队来说，希望对我们的挣扎和胜利的描述将提供一些有用的指示。 一路顺风！</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Airbnb是一个双向市场：hosts房东 vs guests房客 。</p><p>搜索排序模型最早是人工设计的打分函数，就是使用了策略。后来使用了GBDT模型替换了这个策略，Airbnb 的房屋预定得到了大幅度的提升，于是就迭代优化了很多次。</p><p>本文的搜索排名只是 Airbnb 模型生态中的一部分，所有的模型最后的目标都是给客户呈现一个最优的房屋预定列表。生态中的模型有一些是预测房东接受客人预定的概率，一些是预测客人在体验上给五星的概率等。本论文只讨论搜索排名的模型，这个模型负责根据客户预定房屋的可能性给房屋检索列表做一个最优的排序。</p><p>一个成功的搜索会话是以客户开始搜索为开始，以客户预定房屋成功为结束。</p><p>本文主要从以下几个方面展开：</p><ol><li>总结过去一段时间模型架构是如何演变的；</li><li>特征工程和系统工程的思考；</li><li>介绍一些内部工具和超参数方面的探索；</li><li>总结回顾。</li></ol><h1 id="2-Model-Evolution-模型演化"><a href="#2-Model-Evolution-模型演化" class="headerlink" title="2 Model Evolution 模型演化"></a>2 Model Evolution 模型演化</h1><h2 id="2-1-Simple-NN"><a href="#2-1-Simple-NN" class="headerlink" title="2.1 Simple NN"></a>2.1 Simple NN</h2><p>单隐层的NN，32个全连接单元，ReLU激活函数。</p><p>NN的输入特征和GBDT模型一样，训练的目标函数都是最小化 L2 损失函数，都是与GBDT保持一致。</p><h2 id="2-2-Lambdarank-NN"><a href="#2-2-Lambdarank-NN" class="headerlink" title="2.2 Lambdarank NN"></a>2.2 Lambdarank NN</h2><p>将NN和Lambdarank结合使我们首次得到突破。离线我们使用NDCG作为主要指标，因此在NN模型中可以直接优化NDCG，相比于之前的简单NN会有两个重要的改进：</p><ul><li>构造{booked listing, not-booked listing}这个pair对作为训练样本集。训练过程中，最小化预定列表和非预定列表得分之间的交叉熵损失。</li><li>交换两个listing的位置构成一个pair对，然后加权求和。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_discount</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''Apply positional discount curve'''</span></span><br><span class="line">    <span class="keyword">return</span> np.log(<span class="number">2.0</span>)/np.log(<span class="number">2.0</span> + x)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_weights</span><span class="params">(logit_op, session)</span>:</span></span><br><span class="line">    <span class="string">'''Compute loss weights based on delta ndcg.</span></span><br><span class="line"><span class="string">    logit_op is a [BATCH_SIZE, NUM_SAMPLES] shaped tensor</span></span><br><span class="line"><span class="string">    corresponding to the output layer of the network.</span></span><br><span class="line"><span class="string">    Each row corresponds to a search and each column a listing in the</span></span><br><span class="line"><span class="string">    search result. Column 0 is the booked listing, while columns 1 through</span></span><br><span class="line"><span class="string">    NUM_SAMPLES - 1 the not-booked listings. '''</span></span><br><span class="line">    logit_vals = session.run(logit_op)</span><br><span class="line">    ranks = NUM_SAMPLES - <span class="number">1</span> - logit_vals.argsort(axis=<span class="number">1</span>)</span><br><span class="line">    discounted_non_booking = apply_discount(ranks[:, <span class="number">1</span>:])</span><br><span class="line">    discounted_booking = apply_discount(np.expand_dims(ranks[:, <span class="number">0</span>], axis=<span class="number">1</span>))</span><br><span class="line">    discounted_weights = np.abs(discounted_booking - discounted_non_booking)</span><br><span class="line">    <span class="keyword">return</span> discounted_weight</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the pairwise loss</span></span><br><span class="line">pairwise_loss = tf.nn.sigmoid_cross_entropy_with_logits(targets=tf.ones_like(logit_op[:, <span class="number">0</span>]), logits=logit_op[:, <span class="number">0</span>] - logit_op[:, i:] )</span><br><span class="line"><span class="comment"># Compute the lambdarank weights based on delta ndcg</span></span><br><span class="line">weights = compute_weights(logit_op, session)</span><br><span class="line"><span class="comment">#Multiply pairwise loss by lambdarank weights</span></span><br><span class="line">loss = tf.reduce_mean(tf.multiply(pairwise_loss, weights))</span><br></pre></td></tr></table></figure><h2 id="2-3-Decision-Tree-Factorization-Machine-NN"><a href="#2-3-Decision-Tree-Factorization-Machine-NN" class="headerlink" title="2.3 Decision Tree/Factorization Machine NN"></a>2.3 Decision Tree/Factorization Machine NN</h2><p>受到Deep &amp; Cross Network for Ad Click Predictions，也就是DCN的启发，新模型融合了决策树、因子分解机、神经网络三者的优点。 将FM预测的结果作为一个特征加入NN，将GBDT的每棵树的叶子节点index作为一个类别特征加入NN。</p><p><img src="https://static.geekbang.org/infoq/5bf04131e8bce.png?imageView2/0/w/800" alt></p><h2 id="2-4-DeepNN"><a href="#2-4-DeepNN" class="headerlink" title="2.4 DeepNN"></a>2.4 DeepNN</h2><p>在最后的尝试中，我们放弃了所有的复杂模型，精简到使用10倍训练数据来训练一个两个隐层的DNN模型。网络架构如下：</p><ul><li>输入层：195 个类别特征做 embedding</li><li>第一个隐层： 127 个全连接 ReLU</li><li>第二个隐层： 83 个全连接 ReLU</li></ul><blockquote><p>至于为啥隐层数目是127、83这样的数字而不是传统的128、64这种2的幂数，我很好奇的问过作者，得到的回复是：I like prime number.</p><p>We later tested Lambdarank vs pairwise loss in isolation and found them neutral in bookings. So we deprecated Lambdarank and now use the pairwise loss as it is much simpler and faster.</p></blockquote><p>包含的特征有：</p><ul><li>简单的属性特征：价格、酒店设施、历史预定统计等</li><li>Smart Pricing：<em>Customized Regression Model for Airbnb Dynamic Pricing</em></li><li>Similarity of listing：<em>Real-time Personalization using Embeddings for Search Ranking at Airbnb</em></li></ul><h1 id="3-Failed-Models"><a href="#3-Failed-Models" class="headerlink" title="3 Failed Models"></a>3 Failed Models</h1><h2 id="3-1-Listing-ID"><a href="#3-1-Listing-ID" class="headerlink" title="3.1 Listing ID"></a>3.1 Listing ID</h2><p>每个Listing都有其相对应的唯一ID，使用Listing id作为特征。</p><p>将高基数的类别特征做Embedding，在很多应用上取得了成功，如NLP中单词的Embedding的应用，谷歌的推荐系统中用户id和视频id的Embedding。</p><p>但是用Listing ids做Embedding特征导致了过拟合问题，究其原因是由于数据量不足。即使是最受欢迎的Listing，也可以在一年中最多预订365次，而且每个Listing的典型预订量要少得多。</p><p>NLP中单词的Embedding时，word可以无限制的重复；谷歌的推荐系统中用户id和视频id的同样也是可以不断重复出现。但是Airbnb的独特业务性质，导致了该方案的失败。</p><h2 id="3-2-Multi-task-learning"><a href="#3-2-Multi-task-learning" class="headerlink" title="3.2 Multi-task learning"></a>3.2 Multi-task learning</h2><p>bookings有物理限制，但是用户浏览listing详情页没有限制。进一步发现，用户长时间的浏览listing详情页和bookings是正相关的。</p><p>为了解决Listing ids过拟合的问题，建立了多任务学习的模型，同时预测一间房间booking的概率和长时间浏览的概率。最重要的是，Listing id的Embedding输入隐层后，在网络模型中进行了共享。因此我们这样做的动机是希望能够从长时间浏览中学习的信息来预测booking，避免过拟合。</p><p>由于长时间浏览的label是远超过bookings用户几个数量级的，在booking loss上使用了更高的权重作为补偿。后面将长时间浏览的label调整为 <em>log(view duration)</em>  ，在线打分时，我们仅使用了预测booking。</p><p>长时间浏览的数据可能由于高端和高价格的listings所主导，</p><p>关于listing的浏览后续会作为一个专题进行深入研究。</p><h1 id="4-特征工程"><a href="#4-特征工程" class="headerlink" title="4 特征工程"></a>4 特征工程</h1><p>刚开始baseline使用的GBDT做了大量的特征工程，典型的特征转换，包括计算比率，窗口滑动平均等等。</p><h2 id="4-1-Feature-normalization"><a href="#4-1-Feature-normalization" class="headerlink" title="4.1 Feature normalization"></a>4.1 Feature normalization</h2><p>刚开始时用与GBDT相同的输入特征来训练NN，效果很差，因为树模型对特征的大小不敏感，而神经网络的输入需要进行归一化。决策树对数值型特征的大小并不是很重要，只要表征有序就可以，而神经网络对此很敏感。如果输入特征的数值超过通常特征值的范围，在做反向传播计算的时候，就会引起大的梯度改变。由于梯度消失，会导致像 ReLU 这样的激活函数处于永久关闭状态。为了避免这个现象的发生，我们要保证所有特征的值域在一个小的范围内变化。通常的做法是让特征的分布值域在｛-1,1｝，中心点映射到 0。</p><ul><li>正态分布的特征进行中心归一化，即 <em>(feature_val − µ)/σ</em></li><li>幂律分布的特征进行log归一化，即 <em>log((1+feature_val)/(1+median))</em></li></ul><blockquote><p>We wanted the feature to be evenly distributed around 0. Putting the median in the denominator ensures that. The offset of 1 is to equalize the offset of 1 in the numerator.</p></blockquote><blockquote><p>power law distribution:</p></blockquote><h2 id="4-2-Feature-distribution"><a href="#4-2-Feature-distribution" class="headerlink" title="4.2 Feature distribution"></a>4.2 Feature distribution</h2><p>除了将特征映射到一个限制的数值范围，本文确保绝大部分的特征服从平滑的分布。原因如下：</p><ul><li>定位异常 ( Spotting bugs）：在处理数以亿计的特征样本时，我们如何验证它们中的一小部分没有错误？ 范围检查很有用但有限。 我们发现分布的平滑性是发现错误的宝贵工具，因为错误的分布通常与典型的分布不同。 举个例子，我们在某些地区的价格记录中，存在与市价明显不一致的错误。 这是因为在这些地区，对于超过28天的期间，记录的价格是每月价格而不是每日价格。 这些错误表现为分布图上的尖峰。</li><li>提升泛化（Facilitating generalization）：根据我们应用DNNs进行观察时所积累的经验，输出层的分布会逐渐变得越来越平滑。图 8 画出了最后一层输出的分布，图 9和图 10 分别展示了第一层和第二层的分布</li></ul><p><img src="https://static.geekbang.org/infoq/5bf041327b30e.png?imageView2/0/w/800" alt></p><p>我们如何测试模型在登录样本的泛化效果很好呢？在调试并应用适当的标准化时，大多数特征都获得了平滑分布，但有一些我们不得不做专门的特征工程。有个例子是listing的地理位置，由经度和纬度表示。为了使地理特征分布更平滑，通过计算与中心点的偏移量来表征地理特征信息。</p><p><img src="https://static.geekbang.org/infoq/5bf041339a8e0.png?imageView2/0/w/800" alt></p><ul><li><p>检查特征完整性（Checking feature completeness）：某些特征的分布不平滑，会导致模型学习信息缺失。有个例子是房屋未来可以被占用的天数。调查发现列表中有一些房屋有最低的住宿要求，可能延长到几个月。但是我们没有在模型中添加最小所需停留时间，因为它取决于日历并且过于复杂。 但是，如果考虑入住率分布，我们添加了listing平均居住时长作为模型的一个特征。</p><p>在一个维度上缺乏平滑分布的一些特征可能在更高维度上变得平滑。 如果这些维度已经可用于模型，那么我们有必要仔细思考，如果没有，那么添加它们。</p></li></ul><h2 id="4-3-High-cardinality-categorical-features"><a href="#4-3-High-cardinality-categorical-features" class="headerlink" title="4.3 High cardinality categorical features"></a>4.3 High cardinality categorical features</h2><p>过拟合的Listing ID不是我们尝试的唯一高基数类别特征。对于NN而言，我们还有其他一些尝试，通过很少的特征工程获得了高回报。 通过一个具体的例子最好证明这一点。 客人对一个城市的各个临近城市的偏好是一个重要的位置信号。 对于GBDT模型，这些信息由一个设计严密的pipeline提供，该pipeline跟踪社区和城市的预订等级分布。 建设和维护这条pipeline所付出的努力是巨大的。 然而，它并未考虑预订房源价格等关键要素。</p><p>在神经网络中，处理这些信息就很简单了。 我们通过获取查询中指定的城市和与Listing对应的12级S2 cell格来创建新的分类特征。</p><p>发现了地理偏好，比如旧金山西海湾南部的位置比跨越桥梁的位置更受欢迎，后者往往交通拥堵频发。</p><p><img src="https://static.geekbang.org/infoq/5bf0413483c0a.png?imageView2/0/w/800" alt="Location preference learnt for the query &quot;San Francisco&quot;"></p><h1 id="5-系统工程"><a href="#5-系统工程" class="headerlink" title="5 系统工程"></a>5 系统工程</h1><ul><li>Protobufs and Datasets</li><li>Refactoring static features</li><li>Java TM NN library</li></ul><h1 id="6-超参数"><a href="#6-超参数" class="headerlink" title="6 超参数"></a>6 超参数</h1><ul><li>Dropout</li></ul><blockquote><p>Dropout as data augmentation. <a href="https://arxiv.org/abs/1506.08700" target="_blank" rel="noopener">https://arxiv.org/abs/1506.08700</a></p><p>dropout一般被看做很多共享参数模型的集合，即bagging；这篇文章从数据增强角度给予解释，dropout可以看成无领域知识的情况下在输入空间进行数据增强的方法。dropout强迫一个神经单元，和随机挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性，增强了泛化能力。</p></blockquote><ul><li><p>Initialization</p><p>Xavier initialization：$$\text{Var}(W) = \frac{1}{n_\text{in}}$$</p><p>Glorot &amp; Bengio’s paper originally recommended using $$\text{Var}(W) = \frac{2}{n_\text{in} + n_\text{out}}$$</p></li><li><p>Learning rate</p></li></ul><p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/lazy_adam_optimizer.py" target="_blank" rel="noopener">LazyAdamOptimizer</a></p><blockquote><p>Tensorflow has a “<a href="https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/contrib/opt/LazyAdamOptimizer" target="_blank" rel="noopener">Lazy Adam Optimizer</a>“ that only updates the gradient for variables whose indices appear in the current batch. This may be a good idea for very sparse data like language models.</p></blockquote><blockquote><p>Vanilla Adam updates all parameters at every step, while lazy Adam only updates parameters that are actually employed – in a sparse setting like a language model, that can make a big difference, because lazy Adam applies no updates to rare words until they appear, at which time they get a big update. More common words are updated more frequently.</p></blockquote><ul><li>Batch size</li></ul><p><a href="https://medium.com/@malay.haldar/how-much-training-data-do-you-need-da8ec091e956" target="_blank" rel="noopener">How much training data do you need?</a></p><h1 id="7-特征重要性"><a href="#7-特征重要性" class="headerlink" title="7 特征重要性"></a>7 特征重要性</h1><p>总的来说，评估特征重要性和模型可解释性是我们向NN转移的一个领域。评估特征重要性对于优先考虑工程工作和指导模型迭代至关重要。 NN的优势在于理解特征之间的非线性组合。 当理解特定特征扮演什么角色时，这也是一个弱点，因为非线性交互使得单独研究任何特征变得非常困难。 接下来，我们将重述一些破译NN的尝试。</p><ul><li><p>分数分数（Score Decomposition）：我们最初的做法是获取神经网络产生的最终得分，并尝试将其分解为各个节点贡献得分。没有纯净的方法来分离是特定传入节点的影响，还是像ReLU这样的非线性激活函数的影响。</p></li><li><p>消除测试（Ablation Test）：</p></li><li><p>Permutation Test</p></li><li><p>TopBot Analysis：我们自创了一个工具，旨在不以任何方式扰乱特征来解释特征，命名为TopBot，是上下分析器的缩写。将测试集作为输入，并使用模型对每个测试qurey对应的列表进行排序。 然后，它从每个query顶部排名listing中生成特征值的分布图，并将它们与底部listing中的特征值分布进行比较。</p><p>可以看到price在top listing和bottom listing还是有区别的，top listing更倾向于拥有相对小的price，表示模型对价格敏感。而review count却没有任何区别。 从这里，也可以得到一些信息。</p><p><img src="https://static.geekbang.org/infoq/5bf042f95bdf4.png?imageView2/0/w/800" alt="feature importance"></p></li></ul><h1 id="8-回顾总结"><a href="#8-回顾总结" class="headerlink" title="8 回顾总结"></a>8 回顾总结</h1><p>  图15总结了我们迄今为止的深度学习历程。在无处不在的深度学习成功故事的基础上，我们开始处于乐观的高峰期，认为深度学习将成为GBDT模型的替代品，并为我们带来惊人的收益。许多初步讨论都集中在保持其他一切不变，并用神经网络取代当前模型，以了解我们可以获得的收益。这使我们陷入绝望之谷，最初这些收益都没有实现。事实上，我们在开始时看到的只是在线指标的回归。随着时间的推移，我们意识到转向深度学习根本不是替代模型;而是关于扩展系统。因此，它需要重新思考模型周围的整个系统。限制在较小的尺度，像GBDT这样的型号可以说性能相当，更容易处理，我们继续将它们用于集中的中型问题。</p><p>  那么我们是否会向其他人推荐深度学习？那将是一个全心全意的。而且这不仅是因为该模型在线表现的强劲增长。其中一部分与深度学习如何改变我们的路线图有关。早期的重点主要是功能工程，但在深入学习之后，尝试手动对功能进行更好的数学运算已经失去了光彩。这使我们能够在更高层次上解决问题，例如我们如何改进优化目标，并且我们是否准确地代表了所有用户？在采用神经网络搜索排名的第一步后两年，我们觉得我们刚刚开始。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/38/e2/383d9bff2837a4931e3703508d7a01e2.jpg&quot; alt=&quot;airbnb&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a hr
      
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://easonlv.github.io/tags/Deep-Learning/"/>
    
      <category term="paper" scheme="http://easonlv.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Spark中ml和mlib的区别</title>
    <link href="http://easonlv.github.io/2017/04/14/Spark%E4%B8%ADml%E5%92%8Cmllib%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://easonlv.github.io/2017/04/14/Spark中ml和mllib的区别/</id>
    <published>2017-04-14T05:30:41.000Z</published>
    <updated>2017-04-14T05:33:09.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文转自 <a href="https://vimsky.com/article/3403.html" target="_blank" rel="noopener">Spark中ml和mllib的区别</a></p></blockquote><h2 id="Spark中ml和mllib的主要区别和联系如下："><a href="#Spark中ml和mllib的主要区别和联系如下：" class="headerlink" title="Spark中ml和mllib的主要区别和联系如下："></a>Spark中ml和mllib的主要区别和联系如下：</h2><ul><li>ml和mllib都是Spark中的机器学习库，目前常用的机器学习功能2个库都能满足需求。</li><li>spark官方推荐使用ml, 因为ml功能更全面更灵活，未来会主要支持ml，mllib很有可能会被废弃(据说可能是在spark3.0中deprecated）。</li><li>ml主要操作的是DataFrame, 而mllib操作的是RDD，也就是说二者面向的数据集不一样。<ul><li>DataFrame和RDD什么关系？DataFrame是Dataset的子集，也就是Dataset[Row], 而DataSet是对RDD的封装，对SQL之类的操作做了很多优化。</li></ul></li><li>相比于mllib在RDD提供的基础操作，ml在DataFrame上的抽象级别更高，数据和操作耦合度更低。</li><li>ml中的操作可以使用pipeline, 跟sklearn一样，可以把很多操作(算法/特征提取/特征转换)以管道的形式串起来，然后让数据在这个管道中流动。大家可以脑补一下Linux管道在做任务组合时有多么方便。</li><li>ml中无论是什么模型，都提供了统一的算法操作接口，比如模型训练都是<code>fit</code>；不像mllib中不同模型会有各种各样的<code>trainXXX</code>。</li><li>mllib在spark2.0之后进入<code>维护状态</code>, 这个状态通常只修复BUG不增加新功能。</li></ul><p>以上就是ml和mllib的主要异同点。下面是ml和mllib逻辑回归的例子，可以对比看一下， 虽然都是模型训练和预测，但是画风很不一样。</p><h2 id="mllib中逻辑回归的例子"><a href="#mllib中逻辑回归的例子" class="headerlink" title="mllib中逻辑回归的例子"></a>mllib中逻辑回归的例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sparse_data = [</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">0.0</span>, SparseVector(<span class="number">2</span>, &#123;<span class="number">0</span>: <span class="number">0.0</span>&#125;)),</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">1.0</span>, SparseVector(<span class="number">2</span>, &#123;<span class="number">1</span>: <span class="number">1.0</span>&#125;)),</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">0.0</span>, SparseVector(<span class="number">2</span>, &#123;<span class="number">0</span>: <span class="number">1.0</span>&#125;)),</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">1.0</span>, SparseVector(<span class="number">2</span>, &#123;<span class="number">1</span>: <span class="number">2.0</span>&#125;))</span><br><span class="line"><span class="meta">... </span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm = LogisticRegressionWithSGD.train(sc.parallelize(sparse_data), iterations=<span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm.predict(array([<span class="number">0.0</span>, <span class="number">1.0</span>]))</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm.predict(array([<span class="number">1.0</span>, <span class="number">0.0</span>]))</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm.predict(SparseVector(<span class="number">2</span>, &#123;<span class="number">1</span>: <span class="number">1.0</span>&#125;))</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm.predict(SparseVector(<span class="number">2</span>, &#123;<span class="number">0</span>: <span class="number">1.0</span>&#125;))</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> os, tempfile</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>path = tempfile.mkdtemp()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lrm.save(sc, path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sameModel = LogisticRegressionModel.load(sc, path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sameModel.predict(array([<span class="number">0.0</span>, <span class="number">1.0</span>]))</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sameModel.predict(SparseVector(<span class="number">2</span>, &#123;<span class="number">0</span>: <span class="number">1.0</span>&#125;))</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> shutil <span class="keyword">import</span> rmtree</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">try</span>:</span><br><span class="line"><span class="meta">... </span>   rmtree(path)</span><br><span class="line"><span class="meta">... </span><span class="keyword">except</span>:</span><br><span class="line"><span class="meta">... </span>   <span class="keyword">pass</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>multi_class_data = [</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">0.0</span>, [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]),</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">1.0</span>, [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]),</span><br><span class="line"><span class="meta">... </span>    LabeledPoint(<span class="number">2.0</span>, [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line"><span class="meta">... </span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = sc.parallelize(multi_class_data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mcm = LogisticRegressionWithLBFGS.train(data, iterations=<span class="number">10</span>, numClasses=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mcm.predict([<span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">0.0</span>])</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mcm.predict([<span class="number">0.8</span>, <span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mcm.predict([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.3</span>])</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="ml中的逻辑回归的例子"><a href="#ml中的逻辑回归的例子" class="headerlink" title="ml中的逻辑回归的例子"></a>ml中的逻辑回归的例子</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bdf = sc.parallelize([</span><br><span class="line"><span class="meta">... </span>    Row(label=<span class="number">1.0</span>, weight=<span class="number">2.0</span>, features=Vectors.dense(<span class="number">1.0</span>)),</span><br><span class="line"><span class="meta">... </span>    Row(label=<span class="number">0.0</span>, weight=<span class="number">2.0</span>, features=Vectors.sparse(<span class="number">1</span>, [], []))]).toDF()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blor = LogisticRegression(maxIter=<span class="number">5</span>, regParam=<span class="number">0.01</span>, weightCol=<span class="string">"weight"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel = blor.fit(bdf)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.coefficients</span><br><span class="line">DenseVector([<span class="number">5.5</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.intercept</span><br><span class="line"><span class="number">-2.68</span>...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mdf = sc.parallelize([</span><br><span class="line"><span class="meta">... </span>    Row(label=<span class="number">1.0</span>, weight=<span class="number">2.0</span>, features=Vectors.dense(<span class="number">1.0</span>)),</span><br><span class="line"><span class="meta">... </span>    Row(label=<span class="number">0.0</span>, weight=<span class="number">2.0</span>, features=Vectors.sparse(<span class="number">1</span>, [], [])),</span><br><span class="line"><span class="meta">... </span>    Row(label=<span class="number">2.0</span>, weight=<span class="number">2.0</span>, features=Vectors.dense(<span class="number">3.0</span>))]).toDF()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mlor = LogisticRegression(maxIter=<span class="number">5</span>, regParam=<span class="number">0.01</span>, weightCol=<span class="string">"weight"</span>,</span><br><span class="line"><span class="meta">... </span>    family=<span class="string">"multinomial"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mlorModel = mlor.fit(mdf)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(mlorModel.coefficientMatrix)</span><br><span class="line">DenseMatrix([[<span class="number">-2.3</span>...],</span><br><span class="line">             [ <span class="number">0.2</span>...],</span><br><span class="line">             [ <span class="number">2.1</span>... ]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mlorModel.interceptVector</span><br><span class="line">DenseVector([<span class="number">2.0</span>..., <span class="number">0.8</span>..., <span class="number">-2.8</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test0 = sc.parallelize([Row(features=Vectors.dense(<span class="number">-1.0</span>))]).toDF()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = blorModel.transform(test0).head()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result.prediction</span><br><span class="line"><span class="number">0.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result.probability</span><br><span class="line">DenseVector([<span class="number">0.99</span>..., <span class="number">0.00</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result.rawPrediction</span><br><span class="line">DenseVector([<span class="number">8.22</span>..., <span class="number">-8.22</span>...])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>test1 = sc.parallelize([Row(features=Vectors.sparse(<span class="number">1</span>, [<span class="number">0</span>], [<span class="number">1.0</span>]))]).toDF()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.transform(test1).head().prediction</span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blor.setParams(<span class="string">"vector"</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ...</span><br><span class="line">TypeError: Method setParams forces keyword arguments.</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lr_path = temp_path + <span class="string">"/lr"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blor.save(lr_path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lr2 = LogisticRegression.load(lr_path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lr2.getMaxIter()</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model_path = temp_path + <span class="string">"/lr_model"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.save(model_path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model2 = LogisticRegressionModel.load(model_path)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.coefficients[<span class="number">0</span>] == model2.coefficients[<span class="number">0</span>]</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>blorModel.intercept == model2.intercept</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文转自 &lt;a href=&quot;https://vimsky.com/article/3403.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spark中ml和mllib的区别&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;
      
    
    </summary>
    
      <category term="spark" scheme="http://easonlv.github.io/categories/spark/"/>
    
    
      <category term="机器学习" scheme="http://easonlv.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="spark" scheme="http://easonlv.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>从晋北到晋中 Day3</title>
    <link href="http://easonlv.github.io/2017/04/10/%E4%BB%8E%E6%99%8B%E5%8C%97%E5%88%B0%E6%99%8B%E4%B8%AD-Day3/"/>
    <id>http://easonlv.github.io/2017/04/10/从晋北到晋中-Day3/</id>
    <published>2017-04-10T14:00:00.000Z</published>
    <updated>2017-04-10T15:10:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从晋北到晋中"><a href="#从晋北到晋中" class="headerlink" title="从晋北到晋中"></a>从晋北到晋中</h1><h2 id="Day3-平遥古城-2107-4-3"><a href="#Day3-平遥古城-2107-4-3" class="headerlink" title="Day3 平遥古城 (2107.4.3.)"></a>Day3 平遥古城 (2107.4.3.)</h2><p><img src="http://c2-q.mafengwo.net/s10/M00/F3/98/wKgBZ1jo36eAPV7jAA5NhBJWOhA76.jpeg" alt="平遥古城"></p><p>很多年前我从一个电视节目中看到平遥古城，第一次看到还是蛮震撼的，因此在心中一直有想去古城走走的情节。我有去过很多个古镇，皖南的、苏州的、成都的，还有北方的古北水镇，正是习惯了南方的小桥流水和那般精致，才愈发觉得北方的古城给人一种稳重厚实的感觉，平遥古城也是这样一个地方。</p><p>傍晚时分到达平遥古城东门不远的一个宾馆里，放下行李就饥肠辘辘的直奔古城了。昏黄的灯光下，沿路两边的叫卖声，形形色色的游客和熙熙攘攘的车辆，走在石板路上，漫无目的逛了很久，才找了家饭馆吃饭。之前有听说平遥古城内的美食小吃不是一般的难吃，特意选了一家店大人多的，要了平遥特色的各种栲栳栳、长山药等，结果上菜后懵逼了，无色无味难看难吃到无力吐槽，勉强吃了几口逃之夭夭……</p><p>随后逛了逛南大街和西大街，这里是古城内最为繁盛的传统商业街。跟所有旅游区一样，老字号与传统名店铺林立，琳琅满目的工艺品店、美食、酒吧、路边小商贩、特色的足浴室、还有北京玩剩下的VR体验馆等等，文艺气息似乎绝不输给外面世界的任何一个地方。</p><p>平遥酒吧一条街没有九寨风情街那样的震耳欲聋，所以走在路上能清楚的听到那些驻唱歌手的歌声都很好听。“我在平遥等你，任凭时光老去······”，街旁的酒吧里，随着有节奏的手鼓声，传来一首平遥之恋的小曲。我在平遥等你是一家有情调的寄明信片的店儿，在古城几条街上看见多次，门外合影的妹子是真多。据说这儿可以寄存你给未来的自己写的信，也难怪这里最受文艺青年的青睐了。</p><p>在平遥古城住了一晚，第二天早上起床发现外面下雨了，这个假期还是没有逃掉下雨天，心里有点失望，还没见过阳光明媚蓝天白云下古城慢节奏生活的我，又怎会相信下雨天的古城会有另一番韵味……白天走在城里，视野要开阔很多，遇到下雨天，石板路面很滑，积水很多，鞋子很容易湿。因为只有半天的时间，没法走遍古城纵横交错的四大街、八小街、七十二条蚰蜒巷，甚至连几处出名的古城景点都不愿去找了，索性不再脚步匆匆，避开喧嚣的主干道，走向羊肠小道。总觉得相比丽江和大理的浪漫，平遥古城则低调深沉，充满沧桑，随着岁月的洗涤，破旧的民居年久失修，残砖断瓦，枯木石雕是消失的财富帝国的遗迹，让人满目苍夷。</p><p>以前平遥进城收200元门票，城内景点随意观赏，现在进城不需要门票，但是古城里的景点有一张130元的联票，可以去著名的日升昌、县衙、登古城墙……现在的古城越来越商业，越来越喧嚣，很难找到与世无争的安宁，但还是比钢铁丛林的城市森林静谧。在这之前我想象中的平遥古城应该是很多个乔家大院组成的，然后走进城里才发现这儿似乎是多个南锣鼓巷加北锣鼓巷的乡村版。</p><p>今天的早饭和午餐也都是在城里吃的，不过对平遥美食已经没有很大的期待了，果断从大众点评上找，平遥最出名的应该还是特色面食栲栳栳和牛肉等，而且消费真的不高。而且在这里我注意到支付方式都是现金、微信或支付宝的转账，没有银联支付和线下移动支付，互联网行业在三四线小城还没有渗透。就像我在宾馆住宿时，老板说的，平遥要是消费提高了，就没人来了，不知道这是不是一种悲哀。</p><p>中午12点时启程回京了，从平遥回北京，600多公里，预计八点到京。高速路上，一路尽有清明时节雨纷纷，耳畔回荡着歌声: 开车行驶在公路无际无边，有离开自己的错觉……今天是返程高峰，外面还下雨，车内很黑暗，一觉醒来，发现同行的98年出生的清华建筑系女生，用手机看PDF书籍，划线和高亮做注解，再一觉醒来，她手机开着手电筒，看了一路打印的讲义，旅行和学习，始终在路上，这才是应有的生活态度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从晋北到晋中&quot;&gt;&lt;a href=&quot;#从晋北到晋中&quot; class=&quot;headerlink&quot; title=&quot;从晋北到晋中&quot;&gt;&lt;/a&gt;从晋北到晋中&lt;/h1&gt;&lt;h2 id=&quot;Day3-平遥古城-2107-4-3&quot;&gt;&lt;a href=&quot;#Day3-平遥古城-2107-4-3&quot;
      
    
    </summary>
    
      <category term="旅行" scheme="http://easonlv.github.io/categories/%E6%97%85%E8%A1%8C/"/>
    
    
      <category term="风景" scheme="http://easonlv.github.io/tags/%E9%A3%8E%E6%99%AF/"/>
    
      <category term="摄影" scheme="http://easonlv.github.io/tags/%E6%91%84%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>从晋北到晋中 Day2</title>
    <link href="http://easonlv.github.io/2017/04/10/%E4%BB%8E%E6%99%8B%E5%8C%97%E5%88%B0%E6%99%8B%E4%B8%AD-Day2/"/>
    <id>http://easonlv.github.io/2017/04/10/从晋北到晋中-Day2/</id>
    <published>2017-04-10T13:00:00.000Z</published>
    <updated>2017-04-10T15:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从晋北到晋中"><a href="#从晋北到晋中" class="headerlink" title="从晋北到晋中"></a>从晋北到晋中</h1><h2 id="Day2-云冈石窟→乔家大院-2107-4-3"><a href="#Day2-云冈石窟→乔家大院-2107-4-3" class="headerlink" title="Day2 云冈石窟→乔家大院 (2107.4.3.)"></a>Day2 云冈石窟→乔家大院 (2107.4.3.)</h2><p><img src="http://n1-q.mafengwo.net/s10/M00/F2/10/wKgBZ1jo3ZqAZ9CgAA1sD1_e4LM54.jpeg" alt="乔家大院"></p><p>早上6点半起床，7点出发，走S339省道去云冈石窟，大同这个季节的早上还是有点冷的，需要多穿衣服。</p><p>简单介绍下云冈石窟，位于大同市城西约16公里的武周山，石窟依山开凿，是世界文化遗产，也是中国四大石窟之一。参观云冈石窟更像是在阅读一段北魏的历史，迄今1500多年，也就是追溯到公元386年到534年间左右的鲜卑族建立起的北魏王朝。鲜卑是游牧民族，起源于今天的黑龙江额尔古纳河上游与大兴安岭一带，两汉之际开始从东北向南迁徙，在不断的征战中逐步扩大自己的势力范围。公元398年北魏迁都平城（也就是大同），由于佛教的发展及北魏皇帝崇信佛教，开凿云冈石窟。后北魏继续南迁洛阳后，兴建了龙门石窟。</p><p>经过历时的洗礼，现在的景区更像是一座公园，从偌大的停车场到售票厅，到新建的庙宇，真正的二十窟，还有博物馆。参观时从第一窟到第二十窟，但是真正修建的时间是从第二十窟到第一窟的，即早期、中期和晚期石窟。前二窟很小但是聚集了很多人，刚进来大家都很好奇吧，这里很多窟是禁止拍照的，中间有三窟正在维护暂停观赏，第二十窟是最大也是最早修建的。云冈石窟重在雕刻，其石像上面的彩色都是清朝时期工匠所为，其他详细的可参看<a href="http://www.yungang.org/" target="_blank" rel="noopener">官网</a>的介绍。整个园区环境清幽，不算太大，两个小时足够，对于这类人文历史景观，建议大家在参观时几个人一起请个导游讲解比较好。</p><p>上午10点半离开景区，上车时有个插曲，早上出门将未拆封的一瓶纯净水倒入自己的水杯中，可能太过匆忙，杯盖没有扭紧，一杯水流湿了整个书包。书包里的外套全湿，纸巾吸了不少水，因此iPad万幸没有湿。</p><p>离开大同，上高速G55，一路向南，赶往乔家大院和平遥古城。</p><p>在三天假期的第二天中午，我从晋北到晋中。</p><p>汽车行驶在这片黄土高原的高速路上，地势海拔也渐渐从一千米降到几百米。北方的天空虽然是蓝天，却是那种依稀的淡淡的蓝，空气中始终有些雾阻碍视野。</p><p>车窗外是一片灰色，让我想起南方的春暖花开是一片绿色，蓝天下沐浴春风和阳光，我想回家乡了。</p><p>下午四点到达晋中祁县的乔家大院。乔家大院的出名不知道是否与电影《大红灯笼高高挂》和电视剧《乔家大院》有关，但这座建立于清乾隆间的雄伟壮观的建筑群体，确实设计之精巧，工艺之精细，传闻“皇家有故宫，民宅看乔家”，当然有那么一点夸张的成分。刚走进乔家大院时，以为是走进了博物馆，当你细细观察那奇特的建筑风格，砖瓦磨合，斗拱飞檐，彩饰金装，砖石木雕，像我这个非建筑系毕业的都感叹到精工细做和工艺精湛。当然，乔家大院并不是让你来欣赏建筑的，每个院子每间屋子或多或少有它的故事，关于乔氏几代人家族兴衰的历史。按照今天编剧的写法，这里是会有富二代少爷吸毒而亡，刁蛮千金小姐爱炫富引来绑匪且被撕票等狗血情节的……</p><p>乔家大院不算特别大，全部参观完在2个小时内足够，但是方向感不强的人可能很快被绕晕。夕阳西下，听着广场上播放的电视剧中的那些主题曲，看着周边还在不断翻建的新房子，总觉得这处静谧的大院本该低调却因过渡开发而变得如此喧嚣。</p><p>六点钟，从乔家大院离开，去往平遥古城，我最期待的地方。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从晋北到晋中&quot;&gt;&lt;a href=&quot;#从晋北到晋中&quot; class=&quot;headerlink&quot; title=&quot;从晋北到晋中&quot;&gt;&lt;/a&gt;从晋北到晋中&lt;/h1&gt;&lt;h2 id=&quot;Day2-云冈石窟→乔家大院-2107-4-3&quot;&gt;&lt;a href=&quot;#Day2-云冈石窟→乔家大院
      
    
    </summary>
    
      <category term="旅行" scheme="http://easonlv.github.io/categories/%E6%97%85%E8%A1%8C/"/>
    
    
      <category term="风景" scheme="http://easonlv.github.io/tags/%E9%A3%8E%E6%99%AF/"/>
    
      <category term="摄影" scheme="http://easonlv.github.io/tags/%E6%91%84%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>从晋北到晋中 Day1</title>
    <link href="http://easonlv.github.io/2017/04/10/%E4%BB%8E%E6%99%8B%E5%8C%97%E5%88%B0%E6%99%8B%E4%B8%AD-Day1/"/>
    <id>http://easonlv.github.io/2017/04/10/从晋北到晋中-Day1/</id>
    <published>2017-04-10T12:06:02.000Z</published>
    <updated>2017-04-14T05:37:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从晋北到晋中"><a href="#从晋北到晋中" class="headerlink" title="从晋北到晋中"></a>从晋北到晋中</h1><h2 id="Day1-北岳恒山-2107-4-2"><a href="#Day1-北岳恒山-2107-4-2" class="headerlink" title="Day1 北岳恒山 (2107.4.2.)"></a>Day1 北岳恒山 (2107.4.2.)</h2><blockquote><p>“人间四月芳菲尽，蕾红叶绿四季春。”今年的春天来的有点晚，这个时节的北方依然是春寒料峭。关于行走山西的计划，一直在我笔记本的List上。平日一直忙于工作，假期去哪也未曾想过，所以临时起意去山西，主要还是北京到山西比较近，更好的利用三天假期时间。清明时节，没有雨纷纷。4月1日在公司里继续忙碌完一天下班后，去超市购物回家后，收拾行李，准备第二天一早的行程。  </p></blockquote><blockquote><p>放假前一天才开始准确行程，略显仓促，没有传说中的说走就走那么惬意。现在每一次的旅行对于我来说，不愿在行走之前的工作日去花时间做计划，那会打乱工作的节奏，相反，我会选择在旅行过程中忘掉工作，随意而行，期待未知的风景。  </p></blockquote><blockquote><p>没有华丽或雅致的文字，没有绝美或文艺的照片，仅为记录这次旅途中的点点滴滴。</p></blockquote><p><img src="http://c4-q.mafengwo.net/s10/M00/43/20/wKgBZ1jjxTeAJopPAAUnGn0YWuQ28.jpeg" alt="heng-mountain"></p><p>山西，太行山之西，历史文化悠久，地理上属于山地高原。对于我这样的南方人，上学时很少遇到山西的同学，来北京工作后倒是遇到不少，因此所了解到的山西比较出名的旅游景点莫过于恒山、五台山、云冈石窟、平遥古城和乔家大院这些了。</p><p>五岳中有个形容是东岳泰山之雄，西岳华山之险，北岳恒山之幽，中岳嵩山之峻，南岳衡山之秀。如果用一个字形容恒山，就是幽，是指恒山的幽静。恒山位于大同市浑源县境内，自古即为道教圣地，所以山上以寺庙为核心。</p><p>下午1点半到了恒山脚下的悬空寺，由于人太多，没有去登寺，远远望了两眼直奔恒山正门，也算是一个遗憾。恒山上山的路比较好走，没有很多的盘山公路，所以开车很快就能到半山腰处的大门，这里已经有海拔一千五百米左右（晋北地势较高，海拔均在一千米左右），所以从大门检票到登顶，其实也就累积上升几百米的高度，因此爬行难度系数低。</p><p>大概2点时下车稍作休息吃了点东西，平缓了下身体，开始登山。登山的路一直都是在平稳上升，没有曲折的路段，也没有之前爬山遇到的绝望坡、九十九道弯、百步云梯等险峻的台阶。据说后线山道较险，路边绝壁如墙，沿途自然景观极佳，是驴友的最爱。</p><p>这个季节在南方已经春暖花开，然而在北方早晚还是很冷的。恒山上沿途有很多积雪，远处的群山顶上也有大片积雪如同雪山。网上看到图文说登山最好的时节是五六月，山上花开茂盛，绿意盎然，而不像现在还几乎是光秃秃的一片。下午3:45就到了山顶主峰天峰岭，海拔2017米，这儿聚集好多人在排队和主碑合影留念。我们在上面停留了一会儿4点时开始下山，45分钟到达起点。从用时来看，上山2小时，下山1小时，足矣。</p><p>5点多出发离开恒山，奔赴大同市里，一个半小时候后到达酒店——魏都生态乐园酒店。作为煤矿资源发达的城市，大同的路上重卡车太多，如果自驾游需要注意了，担心大车盲区。我们住的四星级酒店紧挨着高速出口，类似于城郊区，离市中心很多路，以至于晚上都没有地方吃饭。最近的中信广场走过去花了有40分钟，一开始不知道会有这么多路，在滴滴叫不到车的情况下做了个错误的决定。后来才知道大同没有滴滴快车，只有出租车，不知道是不是出于地方保护政策。</p><p>八点多的城里，很多店铺都关门了，路上很安静，这对在北京习惯夜生活的我们来说有点不适应。在市里找到一家孟记面馆，不免俗套的要一份大同刀削面，才十块钱，面的味道还行，没让人失望。吃完饭在旁边的小超市买点吃的，然后打车回去，出租车漫天要价，于是第一辆车我拒绝了，叫了很久才叫到第二辆车。跟师傅聊天，问沿途的这些大烟囱和白色排放物，以及空气中弥漫的二氧化硫味。师傅告知那是大同第二发电厂，专为北京输电，飘出的是水蒸气不是污染物。还有大同的煤虽很出名，但都以低价卖给河北了，后来要债都要不回来……师傅还说这个季节有点冷，大同没啥好看的，要等到六月份来才行…师傅没有打表，自己估算了下路程，然后让给他转账，可能在大同这样一座小城里，任意两个地点之间的路程，对师傅来说经验比工具更为准确和方便。</p><p>回到酒店早早躺下，在山西的第一天结束。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;从晋北到晋中&quot;&gt;&lt;a href=&quot;#从晋北到晋中&quot; class=&quot;headerlink&quot; title=&quot;从晋北到晋中&quot;&gt;&lt;/a&gt;从晋北到晋中&lt;/h1&gt;&lt;h2 id=&quot;Day1-北岳恒山-2107-4-2&quot;&gt;&lt;a href=&quot;#Day1-北岳恒山-2107-4-2&quot;
      
    
    </summary>
    
      <category term="旅行" scheme="http://easonlv.github.io/categories/%E6%97%85%E8%A1%8C/"/>
    
    
      <category term="风景" scheme="http://easonlv.github.io/tags/%E9%A3%8E%E6%99%AF/"/>
    
      <category term="摄影" scheme="http://easonlv.github.io/tags/%E6%91%84%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>一书一世界</title>
    <link href="http://easonlv.github.io/2017/03/26/%E4%B8%80%E4%B9%A6%E4%B8%80%E4%B8%96%E7%95%8C/"/>
    <id>http://easonlv.github.io/2017/03/26/一书一世界/</id>
    <published>2017-03-26T08:34:11.000Z</published>
    <updated>2017-04-10T12:01:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一书一世界"><a href="#一书一世界" class="headerlink" title="一书一世界"></a>一书一世界</h1><h3 id="读《岛上书店》"><a href="#读《岛上书店》" class="headerlink" title="读《岛上书店》"></a>读《岛上书店》</h3><p>岛上书店的原著英文名是The Storied Life of A.J. Fikry，正如英文名直译费克里富有戏剧性的人生，讲述了书店老板A.J.充满故事的生命中，遇到了一些可爱的人，经历着爱与被爱、付出与接受、孤独与陪伴的一生。</p><p>A.J. Fikry住在爱丽丝岛上，一个小岛，他拥有岛上唯一的书店，不过最近他更喜欢喝酒，而不是卖书，因为他在为惨死车祸中的妻子妮可感到悲伤。A.J.和妮可是大学的同学，同样都是研究文学的博士生毕业，这也算是志同道合的一对了。妮可去世后，A.J.的颓废使得书店遭遇危机，屋漏偏逢连夜雨，书店里最宝贵的珍藏本图书《帖木儿》也遭窃了。仿佛命运从未眷顾过他，他的内心沦为了荒岛。</p><p>就在此时，神秘的小女孩玛雅，出现在书店中， 看似A.J.照顾起这个孤儿，其实也是玛雅意外地拯救了陷于孤独绝境中的A.J.，并且成为了连接他和小姨子伊斯梅、警长兰比亚斯、出版社女业务员阿米莉娅之间的纽带。</p><p>阿米莉娅Amelia是一位出版社代表，邀请爱丽丝书店出售本季最新的作品。 A.J.一开始对她并不友好，后来在接触中慢慢心生爱意。这位中年男人不懂的怎样表达，甚至连去她家看望她都很含蓄，连玛雅看着都急，好在最后两个人终在一起。伊斯梅是妮可的姐姐，也就是A.J.的小姨子，可惜遇到了一个花心的丈夫和一个个谎言，最后和警长兰比亚斯心生爱意。兰比亚斯是A.J.的好朋友，职业的敏感让他在伊斯梅家中发现了一些秘密：关于玛雅的身世、那本被盗的书……</p><p>小岛上的主要人物和故事还有很多，但他们的生命紧紧相依，并最终一起走出了人生的困境，遗憾的是A.J.没有战胜肿瘤而离去，书店也有了新的主人，所有对书和生活的热爱都周而复始，愈加汹涌。</p><p>如果你喜欢阅读，喜欢书籍和书店，我相信这是一部能吸引你的小说。这儿有些书中的经典语录。</p><blockquote><p>A.J.：“想要了解一个人，你只需要问一个问题‘你最喜欢哪本书？’”<br>“一旦一个人在乎一件事，就发现自己不得不开始在乎一切事。”</p><p>A.J.向阿米莉亚求婚：“……我可以向你保证有书、有交流，还有我的全心全意，艾米。”</p></blockquote><blockquote><p>兰比亚斯：“有时候别人跟你说你踏上一种旅程，结果却成了另外一种旅程。”<br>“生活中每一桩糟糕事，几乎都是时机不当的结果，每件好事，都是时机恰到好处的结果。”</p></blockquote><blockquote><p>《迟暮开花》：“因为从心底害怕自己不值得被爱，我们独来独往。然而就是因为独来独往，才让我们以为自己不值得被爱。有一天，你不知道是什么时候，你会驱车上路。有一天，你不知道是什么时候，你会遇见他。你会被爱，因为你今生第一次真正不再孤单。你会选择不再孤单下去。”</p></blockquote><p>我是利用早上上班坐地铁的半个多小时和晚上睡觉前躺床上看一小时书的时间，零散且不急不慢地在读。从一开始的情节迷茫、情感复杂到最后的人物心理反思，隐情的慢慢暴露，从开始的颓废到心里充满希望走出困境，小岛上的生活仿佛与世隔绝，而爱是最好的礼物，让一无所有的孤岛成为丰富美丽的存在。</p><p>所以，每一本书，都是一个世界。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一书一世界&quot;&gt;&lt;a href=&quot;#一书一世界&quot; class=&quot;headerlink&quot; title=&quot;一书一世界&quot;&gt;&lt;/a&gt;一书一世界&lt;/h1&gt;&lt;h3 id=&quot;读《岛上书店》&quot;&gt;&lt;a href=&quot;#读《岛上书店》&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="读书" scheme="http://easonlv.github.io/categories/%E8%AF%BB%E4%B9%A6/"/>
    
    
      <category term="读书笔记" scheme="http://easonlv.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Mac系统安装Xgboost</title>
    <link href="http://easonlv.github.io/2017/03/24/Mac%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85Xgboost/"/>
    <id>http://easonlv.github.io/2017/03/24/Mac系统安装Xgboost/</id>
    <published>2017-03-24T14:40:41.000Z</published>
    <updated>2017-03-24T15:01:59.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mac系统安装Xgboost"><a href="#Mac系统安装Xgboost" class="headerlink" title="Mac系统安装Xgboost"></a>Mac系统安装Xgboost</h1><p><a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">Xgboost</a>是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量kaggle选手选用它进行数据挖掘比赛，其中包括两个以上kaggle比赛的夺冠方案。在工业界规模方面，xgboost的分布式版本有广泛的可移植性，支持在YARN, MPI, Sungrid Engine等各个平台上面运行，并且保留了单机并行版本的各种优化，使得它可以很好地解决于工业界规模的问题。</p><p>本文是我今天在Mac系统下成功安（cai）装（keng）Xgboost的笔记（Windows系统用户请出门左转），Mac系统10.12版本按照Xgboost官网<a href="http://xgboost.readthedocs.io/en/latest/build.html" target="_blank" rel="noopener">安装指南</a>，出现了错误。</p><p>如果按照传统Python库用pip来安装：sudo pip install xgboost，同样出现了错误，最后亲测下面方法安装成功。</p><h4 id="1-安装-Homebrew"><a href="#1-安装-Homebrew" class="headerlink" title="1.安装 Homebrew"></a>1.安装 <a href="https://brew.sh/" target="_blank" rel="noopener">Homebrew</a></h4><p>Homebrew是Mac系统下非常优秀的包管理工具，相当于Ubuntu的apt-get，安装命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure><h4 id="2-安装最新版本的gcc，即gcc-6"><a href="#2-安装最新版本的gcc，即gcc-6" class="headerlink" title="2.安装最新版本的gcc，即gcc-6"></a>2.安装最新版本的gcc，即gcc-6</h4><p>Mac系统默认有Python、Ruby、Shell等环境，但是没有gcc和g++的，安装XCode之后会有clang，而XCode自带的clang是不支持OpenMP的，所以需要重新安装gcc。这个过程比较久，一直在下载，网速快的话差不多半个多小时吧。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install gcc --without-multilib</span><br></pre></td></tr></table></figure><h4 id="3-下载Xgboost的git源码"><a href="#3-下载Xgboost的git源码" class="headerlink" title="3.下载Xgboost的git源码"></a>3.下载Xgboost的git源码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/dmlc/xgboost  </span><br><span class="line">cd xgboost</span><br></pre></td></tr></table></figure><h4 id="4-修改配置文件，用于编译"><a href="#4-修改配置文件，用于编译" class="headerlink" title="4.修改配置文件，用于编译"></a>4.修改配置文件，用于编译</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp make/config.mk ./config.mk</span><br></pre></td></tr></table></figure><p>用vim打开config.mk，修改下面两行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CC = gcc-6</span><br><span class="line">export CXX = g++-6</span><br></pre></td></tr></table></figure><p>这样做是为了用之前下载的gcc-6而不是系统已有的gcc来编译。<br>本来按照官网的 make -j4 来编译，但是未成功，所以改用下面这种方法来编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build.sh</span><br></pre></td></tr></table></figure><h4 id="5-安装python版Xgboost"><a href="#5-安装python版Xgboost" class="headerlink" title="5.安装python版Xgboost"></a>5.安装python版Xgboost</h4><p>安装Xgboost的Python版需要Numpy、Scipy等数值计算库，建议安装Anaconda，所有的科学计算和数据挖掘库都安装好了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd python-package</span><br><span class="line">sudo python setup.py install</span><br></pre></td></tr></table></figure><h4 id="6-打开Python验证"><a href="#6-打开Python验证" class="headerlink" title="6.打开Python验证"></a>6.打开Python验证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import xgboost as xgb</span><br></pre></td></tr></table></figure><p>OK，没有报错，安装成功。<br>下面你可以用Xgboost算法来玩数据了，官网有好多<a href="http://xgboost.readthedocs.io/en/latest/python/python_intro.html#install-xgboost" target="_blank" rel="noopener">Example</a>来学习。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="comment"># read in data</span></span><br><span class="line">dtrain = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.train'</span>)</span><br><span class="line">dtest = xgb.DMatrix(<span class="string">'demo/data/agaricus.txt.test'</span>)</span><br><span class="line"><span class="comment"># specify parameters via map</span></span><br><span class="line">param = &#123;<span class="string">'max_depth'</span>:<span class="number">2</span>, <span class="string">'eta'</span>:<span class="number">1</span>, <span class="string">'silent'</span>:<span class="number">1</span>, <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span> &#125;</span><br><span class="line">num_round = <span class="number">2</span></span><br><span class="line">bst = xgb.train(param, dtrain, num_round)</span><br><span class="line"><span class="comment"># make prediction</span></span><br><span class="line">preds = bst.predict(dtest)</span><br></pre></td></tr></table></figure><p>如果你想玩kaggle比赛，Xgboost这把屠龙刀更不可少。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mac系统安装Xgboost&quot;&gt;&lt;a href=&quot;#Mac系统安装Xgboost&quot; class=&quot;headerlink&quot; title=&quot;Mac系统安装Xgboost&quot;&gt;&lt;/a&gt;Mac系统安装Xgboost&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://githu
      
    
    </summary>
    
      <category term="机器学习" scheme="http://easonlv.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://easonlv.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="xgboost" scheme="http://easonlv.github.io/tags/xgboost/"/>
    
  </entry>
  
  <entry>
    <title>spark学习之RDD</title>
    <link href="http://easonlv.github.io/2016/10/03/spark%E5%AD%A6%E4%B9%A0%E4%B9%8BRDD/"/>
    <id>http://easonlv.github.io/2016/10/03/spark学习之RDD/</id>
    <published>2016-10-03T15:47:41.000Z</published>
    <updated>2017-04-10T12:01:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-RDD基础"><a href="#1-RDD基础" class="headerlink" title="1.RDD基础"></a>1.RDD基础</h2><p>Spark对数据的核心抽象——弹性分布式数据集(Resilient Distributed Dataset,简称RDD)。RDD其实就是分布式的元素集合。在 Spark中,对数据的所有操作不外乎创建 RDD、转化已有RDD以及调用RDD操作进行求值。而在这一切背后,Spark会自动将RDD中的数据分发到集群上,并将操作并行化执行。</p><h2 id="2-创建RDD"><a href="#2-创建RDD" class="headerlink" title="2. 创建RDD"></a>2. 创建RDD</h2><p>Spark提供了两种创建 RDD 的方式:  </p><ul><li>读取外部数据集</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;/path/to/README.md&quot;)</span><br></pre></td></tr></table></figure><ul><li>在程序中对一个集合进行并行化</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> lines = sc.parallelize(<span class="type">List</span>(<span class="string">"pandas"</span>, <span class="string">"i like pandas"</span>))</span><br></pre></td></tr></table></figure><h2 id="3-RDD操作"><a href="#3-RDD操作" class="headerlink" title="3. RDD操作"></a>3. RDD操作</h2><p>RDD 支持两种操作: <strong>转化操作</strong> <em>transformation</em> 和 <strong>行动操作</strong> <em>action</em></p><h3 id="3-1-转化操作"><a href="#3-1-转化操作" class="headerlink" title="3.1 转化操作"></a>3.1 转化操作</h3><p>转化操作是返回一个新的 RDD 的操作,比如 map() 和 filter()<br>map() 接收一个函数,把这个函数用于 RDD 中的每个元素, 将函数的返回结果作为结果RDD中对应元素的值。 如用Scala 实现map计算 RDD中各值的平方：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val input = sc.parallelize(List(1, 2, 3, 4))</span><br><span class="line">val result = input.map(x =&gt; x * x)</span><br><span class="line">println(result.collect().mkString(&quot;,&quot;))</span><br></pre></td></tr></table></figure><p>flatMap()和 map() 类似,函数被分别应用到了输入 RDD 的每个元素上,不过返回的不是一个元素,而是一个返回值序列的迭代器。即对每个输入元素生成多个输出元素，得到由各列表中的元素组成的 RDD。<br>filter() 则接收一个函数,并将 RDD 中满足该函数的 元素放入新的 RDD 中返回，如用 Scala 实现 filter() 转化操作:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val inputRDD = sc.textFile(&quot;log.txt&quot;)</span><br><span class="line">val errorsRDD = inputRDD.filter(line =&gt; line.contains(&quot;error&quot;))</span><br></pre></td></tr></table></figure><table><thead><tr><th>函数</th><th style="text-align:left">功能</th><th style="text-align:left">示例</th><th style="text-align:left">结果</th></tr></thead><tbody><tr><td>map()</td><td style="text-align:left">将函数应用于 RDD 中的每个元素,将返回值构成新的RDD</td><td style="text-align:left">rdd.map(x =&gt; x + 1)</td><td style="text-align:left">{2, 3, 4, 4}</td></tr><tr><td>flatMap()</td><td style="text-align:left">将函数应用于 RDD 中的每个元素,将返回的迭代器的所有内容构成新的RDD，通常用来切分单词</td><td style="text-align:left">rdd.flatMap(x =&gt; x.to(3))</td><td style="text-align:left">{1, 2, 3,2, 3, 3, 3}</td></tr><tr><td>filter()</td><td style="text-align:left">返回一个由通过传给 filter() 的函数的元素组成的 RDD</td><td style="text-align:left">rdd.filter(x =&gt; x != 1)</td><td style="text-align:left">{2, 3, 3}</td></tr><tr><td>distinct()</td><td style="text-align:left">去重</td><td style="text-align:left">rdd.distinct()</td><td style="text-align:left">{1, 2, 3}</td></tr><tr><td>sample(withReplacement, fraction, [seed])</td><td style="text-align:left">对 RDD 采样,以及是否替换</td><td style="text-align:left">rdd.sample(false, 0.5)</td><td style="text-align:left">非确定的</td></tr></tbody></table><h3 id="3-2-行动操作"><a href="#3-2-行动操作" class="headerlink" title="3.2 行动操作"></a>3.2 行动操作</h3><p>行动操作则是向程序返回结果或把结果写入外部系统的操作,会触发实际的计算,比如 count() 和 first()<br>在 Scala 中使用行动操作对错误进行计数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">println(&quot;Input had &quot; + badLinesRDD.count() + &quot; concerning lines&quot;)</span><br><span class="line">println(&quot;Here are 10 examples:&quot;)</span><br><span class="line">badLinesRDD.take(10).foreach(println)</span><br></pre></td></tr></table></figure><p>reduce()接收一个函数作为参数,这个函数操作两个RDD的元素类型的数据并返回一个同样类型的新元素。一个简单的例子就是函数 + ,可以用它来对RDD进行累加。<br>Scala 中的 reduce()：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val sum = rdd.reduce((x, y) =&gt; x + y)</span><br></pre></td></tr></table></figure><p>fold() 和 reduce() 类似,接收一个与 reduce() 接收的函数签名相同的函数,再加上一个 “初始值”来作为每个分区第一次调用时的结果。例如 + 对应的 0, * 对应的 1,或拼接操作对应的空列表</p><p> collect() 函数可以用来获取整个 RDD 中的数据，但collect() 不能用在大规模数据集上，仅当整个数据集能在单台机器的内存中放得下时才能使用。<br> saveAsTextFile()、saveAsSequenceFile(),或者任意的其他行动操作来把 RDD 的数据内容以各种自带的格式保存起来。</p><h2 id="4-RDD持久化"><a href="#4-RDD持久化" class="headerlink" title="4. RDD持久化"></a>4. RDD持久化</h2><p>默认情况下RDD的内容是临时的，但Spark提供了在RDD中持久化数据的机制。第一次调用动作并计算出RDD内容后，RDD的内容可以存储在集群的内存或磁盘上。这样下一次需要调用依赖该RDD的动作时，就不需要从依赖关系中重新计算RDD，数据可以从缓存分区中直接返回：</p><blockquote><p>cached.cache()<br>cached.count()<br>cached.take(10)  </p></blockquote><p>在上述代码中，cache方法调用指示在下次计算RDD后，要把RDD存储起来。调用count会导致第一次计算RDD。采取（take）这个动作返回一个本地的Array，包含RDD的前10个元素。但调用take时，访问的是cached已经缓存好的元素，而不是从cached的依赖关系中重新计算出来的。  </p><p>当Spark持久化存储一个RDD 时，计算出 RDD 的节点会分别保存它们所求出的分区数据。如果一个有持久化数据的节点发生故障，Spark会在需要用到缓存的数据时重算丢失的数据分区。Spark为持久化RDD定义了几种不同的机制，用不同的StorageLevel值表示。  </p><p>rdd.cache()是rdd.persist(StorageLevel.MEMORY)的简写，它将RDD存储为未序列化的对象。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-RDD基础&quot;&gt;&lt;a href=&quot;#1-RDD基础&quot; class=&quot;headerlink&quot; title=&quot;1.RDD基础&quot;&gt;&lt;/a&gt;1.RDD基础&lt;/h2&gt;&lt;p&gt;Spark对数据的核心抽象——弹性分布式数据集(Resilient Distributed Data
      
    
    </summary>
    
    
      <category term="spark" scheme="http://easonlv.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Implemention of QuickSork</title>
    <link href="http://easonlv.github.io/2016/09/25/Implemention-of-QuickSork/"/>
    <id>http://easonlv.github.io/2016/09/25/Implemention-of-QuickSork/</id>
    <published>2016-09-24T16:55:25.000Z</published>
    <updated>2017-04-10T12:02:25.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h1><p>###算法过程<br><strong>快速排序</strong> 的思想很简单，整个排序过程只需要三步：</p><ol><li>在数据集之中，找一个基准点；</li><li>建立两个数组，分别存储左边和右边的数组；</li><li>利用递归进行下次比较。</li></ol><h3 id="Java代码"><a href="#Java代码" class="headerlink" title="Java代码"></a>Java代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QuickSort</span> </span>&#123;    <span class="comment">//一次划分</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> pivotKey = arr[left];</span><br><span class="line">        <span class="keyword">int</span> pivotPointer = left;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(left &lt; right) &#123;</span><br><span class="line">            <span class="keyword">while</span>(left &lt; right &amp;&amp; arr[right] &gt;= pivotKey)</span><br><span class="line">                right --;</span><br><span class="line">            <span class="keyword">while</span>(left &lt; right &amp;&amp; arr[left] &lt;= pivotKey)</span><br><span class="line">                left ++;</span><br><span class="line">            swap(arr, left, right); <span class="comment">//把大的交换到右边，把小的交换到左边。</span></span><br><span class="line">        &#125;</span><br><span class="line">        swap(arr, pivotPointer, left); <span class="comment">//最后把pivot交换到中间</span></span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">quickSort</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(left &gt;= right)</span><br><span class="line">            <span class="keyword">return</span> ;</span><br><span class="line">        <span class="keyword">int</span> pivotPos = partition(arr, left, right);</span><br><span class="line">        quickSort(arr, left, pivotPos-<span class="number">1</span>);</span><br><span class="line">        quickSort(arr, pivotPos+<span class="number">1</span>, right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(<span class="keyword">int</span>[] arr)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(arr == <span class="keyword">null</span> || arr.length == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> ;</span><br><span class="line">        quickSort(arr, <span class="number">0</span>, arr.length-<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span>[] arr, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = arr[left];</span><br><span class="line">        arr[left] = arr[right];</span><br><span class="line">        arr[right] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="C代码"><a href="#C代码" class="headerlink" title="C代码"></a>C代码</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Division</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> <span class="comment">//分割</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> base=a[left];    <span class="comment">//基准元素</span></span><br><span class="line">    <span class="keyword">while</span>(left&lt;right)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(left&lt;right &amp;&amp; a[right]&gt;base)</span><br><span class="line">            --right;     <span class="comment">//从右向左找第一个比基准小的元素</span></span><br><span class="line">        a[left]=a[right];</span><br><span class="line">        <span class="keyword">while</span>(left&lt;right &amp;&amp; a[left]&lt;base )</span><br><span class="line">            ++left;      <span class="comment">//从左向右找第一个比基准大的元素</span></span><br><span class="line">        a[right]=a[left];</span><br><span class="line">    &#125;</span><br><span class="line">    a[left]=base;</span><br><span class="line">    <span class="keyword">return</span> left;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">QuickSort</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> left,<span class="keyword">int</span> right)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j;</span><br><span class="line">    <span class="keyword">if</span>(left&lt;right)</span><br><span class="line">    &#123;</span><br><span class="line">        i=Division(a,left,right);   <span class="comment">//分割</span></span><br><span class="line">        QuickSort(a,left,i<span class="number">-1</span>);     <span class="comment">//将两部分分别排序</span></span><br><span class="line">        QuickSort(a,i+<span class="number">1</span>,right);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="JavaScript代码"><a href="#JavaScript代码" class="headerlink" title="JavaScript代码"></a>JavaScript代码</h3><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">quickSort</span>(<span class="params">arr</span>)</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(arr.length&lt;=<span class="number">1</span>)&#123;</span><br><span class="line"><span class="keyword">return</span> arr;<span class="comment">//如果数组只有一个数，就直接返回；</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> num = <span class="built_in">Math</span>.floor(arr.length/<span class="number">2</span>);<span class="comment">//找到中间数的索引值，如果是浮点数，则向下取整</span></span><br><span class="line"><span class="keyword">var</span> numValue = arr.splice(num,<span class="number">1</span>);<span class="comment">//找到中间数的值</span></span><br><span class="line"><span class="keyword">var</span> left = [];</span><br><span class="line"><span class="keyword">var</span> right = [];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">var</span> i=<span class="number">0</span>;i&lt;arr.length;i++)&#123;</span><br><span class="line"><span class="keyword">if</span>(arr[i]&lt;numValue)&#123;</span><br><span class="line">left.push(arr[i]);<span class="comment">//基准点的左边的数传到左边数组</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line">right.push(arr[i]);<span class="comment">//基准点的右边的数传到右边数组</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> quickSort(left).concat(numValue,quickSort(right));<span class="comment">//递归不断重复比较</span></span><br><span class="line">&#125;</span><br><span class="line">alert(quickSort([<span class="number">32</span>,<span class="number">45</span>,<span class="number">37</span>,<span class="number">16</span>,<span class="number">2</span>,<span class="number">87</span>]));<span class="comment">//弹出“2,16,32,37,45,87”</span></span><br></pre></td></tr></table></figure><h3 id="Python代码"><a href="#Python代码" class="headerlink" title="Python代码"></a>Python代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub_sort</span><span class="params">(array,low,high)</span>:</span></span><br><span class="line">    key = array[low]</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[high] &gt;= key:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> array[high] &lt; key:</span><br><span class="line">            array[low] = array[high]</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">            array[high] = array[low]</span><br><span class="line">    array[low] = key</span><br><span class="line">    <span class="keyword">return</span> low</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(array,low,high)</span>:</span></span><br><span class="line">     <span class="keyword">if</span> low &lt; high:</span><br><span class="line">        key_index = sub_sort(array,low,high)</span><br><span class="line">        quick_sort(array,low,key_index)</span><br><span class="line">        quick_sort(array,key_index+<span class="number">1</span>,high)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    array = [<span class="number">8</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">16</span>,<span class="number">5</span>,<span class="number">13</span>,<span class="number">26</span>,<span class="number">18</span>,<span class="number">2</span>,<span class="number">45</span>,<span class="number">34</span>,<span class="number">23</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">3</span>]</span><br><span class="line">    <span class="keyword">print</span> array</span><br><span class="line">    quick_sort(array,<span class="number">0</span>,len(array)<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">print</span> array</span><br></pre></td></tr></table></figure><h3 id="R代码"><a href="#R代码" class="headerlink" title="R代码"></a>R代码</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">qsort &lt;- <span class="keyword">function</span>(v) &#123;</span><br><span class="line">  <span class="keyword">if</span> ( length(v) &gt; <span class="number">1</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    pivot &lt;- (min(v) + max(v))/<span class="number">2.0</span>                            </span><br><span class="line">    c(qsort(v[v &lt; pivot]), v[v == pivot], qsort(v[v &gt; pivot]))</span><br><span class="line">  &#125; <span class="keyword">else</span> v</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">N &lt;- <span class="number">100</span></span><br><span class="line">vs &lt;- runif(N)</span><br><span class="line">system.time(u &lt;- qsort(vs))</span><br><span class="line">print(u)</span><br><span class="line"></span><br><span class="line">Qsort &lt;- <span class="keyword">function</span>(x)&#123;</span><br><span class="line"> <span class="keyword">if</span> (length(x) &lt; <span class="number">2</span>) <span class="keyword">return</span>(x)</span><br><span class="line"> <span class="keyword">return</span>(c(Qsort(x[x&lt;x[<span class="number">1</span>]]), x[x==x[<span class="number">1</span>]], Qsort(x[x&gt;x[<span class="number">1</span>]])))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;快速排序&quot;&gt;&lt;a href=&quot;#快速排序&quot; class=&quot;headerlink&quot; title=&quot;快速排序&quot;&gt;&lt;/a&gt;快速排序&lt;/h1&gt;&lt;p&gt;###算法过程&lt;br&gt;&lt;strong&gt;快速排序&lt;/strong&gt; 的思想很简单，整个排序过程只需要三步：&lt;/p&gt;
&lt;ol&gt;

      
    
    </summary>
    
    
      <category term="数据结构" scheme="http://easonlv.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://easonlv.github.io/2016/09/22/hello-world/"/>
    <id>http://easonlv.github.io/2016/09/22/hello-world/</id>
    <published>2016-09-22T07:14:27.000Z</published>
    <updated>2016-09-22T07:14:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
